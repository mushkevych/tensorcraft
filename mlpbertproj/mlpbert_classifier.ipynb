{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-07T21:59:51.217989Z",
     "start_time": "2025-02-07T21:59:51.215634Z"
    }
   },
   "source": [
    "from os import path\n",
    "\n",
    "PROJECT_ROOT = path.abspath(path.join(globals()['_dh'][0], '..'))\n",
    "DATALAKE_PATH = path.abspath(path.join(PROJECT_ROOT, '..', '..', 'datalake', 'mlpbertproj'))\n",
    "FQFN_PROCESSED_DF = path.join(DATALAKE_PATH, 'processed', f'processed_20250205.jsonl')\n",
    "MODEL_DIR = path.abspath(path.join(PROJECT_ROOT, 'mlpbertproj', 'classifier'))"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-07T21:59:51.388517Z",
     "start_time": "2025-02-07T21:59:51.221289Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_unified = pd.read_json(f'file://{FQFN_PROCESSED_DF}', orient='records', lines=True)\n",
    "print(f'df shape={df_unified.shape}')\n",
    "print(f'df columns={df_unified.columns}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape=(699, 5)\n",
      "df columns=Index(['file_name', 'fqfn', 'text_body', 'text_embeddings', 'label'], dtype='object')\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T21:59:52.774600Z",
     "start_time": "2025-02-07T21:59:51.436253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trainer.mlpbert_trainer import Trainer, TrainerConf\n",
    "\n",
    "# Ensure your DataFrame, df_unified, is loaded and the code_model is initialized\n",
    "trainer = Trainer(df_unified, TrainerConf())"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "from time import time\n",
    "start_time = time()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(f'Training duration: {time() - start_time:.4f} seconds')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-07T21:59:53.324608Z",
     "start_time": "2025-02-07T21:59:52.780278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/ 12, LR:0.00060, Loss:0.23014\n",
      "Epoch:   2/ 12, LR:0.00060, Loss:0.04192\n",
      "Epoch:   3/ 12, LR:0.00059, Loss:0.02775\n",
      "Epoch:   4/ 12, LR:0.00056, Loss:0.00930\n",
      "Epoch:   5/ 12, LR:0.00051, Loss:0.00261\n",
      "Epoch:   6/ 12, LR:0.00044, Loss:0.00100\n",
      "Epoch:   7/ 12, LR:0.00037, Loss:0.00069\n",
      "Epoch:   8/ 12, LR:0.00029, Loss:0.00092\n",
      "Epoch:   9/ 12, LR:0.00022, Loss:0.00036\n",
      "Epoch:  10/ 12, LR:0.00015, Loss:0.00058\n",
      "Epoch:  11/ 12, LR:0.00010, Loss:0.00027\n",
      "Epoch:  12/ 12, LR:0.00007, Loss:0.00020\n",
      "Training duration: 0.5428 seconds\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compute Evaluation Metrics"
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-07T21:59:53.371731Z",
     "start_time": "2025-02-07T21:59:53.331268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from mlpbertproj.classifier.onnx_exporter import EVAL_METRICS_FILE_NAME\n",
    "\n",
    "fqfn_metrics = path.join(MODEL_DIR, EVAL_METRICS_FILE_NAME)\n",
    "eval_metrics = trainer.evaluate(fqfn_metrics)\n",
    "print(f'Evaluation metrics: {json.dumps(eval_metrics, indent=2)}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {\n",
      "  \"accuracy\": 1.0,\n",
      "  \"f1\": 1.0,\n",
      "  \"precision\": 1.0,\n",
      "  \"recall\": 1.0,\n",
      "  \"roc-auc\": 1.0\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "from mlpbertproj.classifier.onnx_exporter import WEIGHTS_FILE_NAME\n",
    "from pt_utils import save_model_weights\n",
    "\n",
    "fqfn_model_save = path.join(MODEL_DIR, WEIGHTS_FILE_NAME)\n",
    "save_model_weights(trainer.model, fqfn_model_save)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-07T21:59:53.512660Z",
     "start_time": "2025-02-07T21:59:53.377317Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# measure inference time"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T21:59:53.558489Z",
     "start_time": "2025-02-07T21:59:53.518531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import timeit\n",
    "\n",
    "from mlpbertproj.classifier.mlpbert_configuration import ModelConf\n",
    "from mlpbertproj.classifier.onnx_exporter import WEIGHTS_FILE_NAME\n",
    "from mlpbertproj.classifier.mlpbert_classifier import MlpBertModel\n",
    "from utils.compute_device import DEVICES\n",
    "\n",
    "print(f'Model Class: {MlpBertModel.__name__}')\n",
    "model_conf = ModelConf()\n",
    "fqfn_model_save = path.join(MODEL_DIR, WEIGHTS_FILE_NAME)\n",
    "trained_model = MlpBertModel(model_conf=model_conf)\n",
    "trained_model.load_model_weights(fqfn_model_save)\n",
    "\n",
    "number_of_iterations = 1000\n",
    "with torch.no_grad():\n",
    "    trained_model(\n",
    "        torch.rand(size=(1, ModelConf.input_size), device=DEVICES['cpu'])\n",
    "    )\n",
    "    timer = timeit.Timer(stmt=lambda: trained_model(\n",
    "        torch.rand(size=(1, ModelConf.input_size), device=DEVICES['cpu'])\n",
    "    ))\n",
    "    times = timer.repeat(repeat=1, number=number_of_iterations)  # repeat=1 to run 1000 iterations once\n",
    "\n",
    "    average_time = sum(times) / (len(times) * number_of_iterations)\n",
    "\n",
    "print(f'Average execution time: {average_time} seconds')\n",
    "print()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 13:59:53,521 - tensorcraft - INFO - XLA Device Not Supported: No module named 'torch_xla'\n",
      "2025-02-07 13:59:53,521 - tensorcraft - INFO - Pytorch version=2.6.0 preferred device=mps build with MPS support=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Class: MlpBertModel\n",
      "Average execution time: 2.970379200996831e-05 seconds\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compute System Metrics"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T21:59:53.567145Z",
     "start_time": "2025-02-07T21:59:53.563723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from mlpbertproj.classifier.onnx_exporter import SYS_METRICS_FILE_NAME\n",
    "from mlpbertproj.classifier.mlpbert_configuration import ModelConf\n",
    "\n",
    "\n",
    "sys_metrics = {\n",
    "    'input_size': ModelConf.input_size,\n",
    "    'avg_inference_sec': round(average_time, 9),\n",
    "    'parameter_count': trainer.model.parameter_count,\n",
    "    'dataset.split_ratio': trainer.trainer_conf.dataset_split_ratio,\n",
    "}\n",
    "\n",
    "for ds in [trainer.test_dataset, trainer.train_dataset]:\n",
    "    for label in ds.df['label'].unique():\n",
    "        labeled_df = ds.df[ds.df['label'] == label]\n",
    "        if f'dataset_size.class_{label}' not in sys_metrics:\n",
    "            sys_metrics[f'dataset_size.class_{label}'] = 0\n",
    "        sys_metrics[f'dataset_size.class_{label}'] += labeled_df.shape[0]\n",
    "\n",
    "fqfn_sys_metrics = path.join(MODEL_DIR, SYS_METRICS_FILE_NAME)\n",
    "with open(fqfn_sys_metrics, 'w+') as metric_file:\n",
    "    json.dump(sys_metrics, metric_file, indent=2)\n",
    "print(f'System metrics: {json.dumps(sys_metrics, indent=2)}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System metrics: {\n",
      "  \"input_size\": 768,\n",
      "  \"avg_inference_sec\": 2.9704e-05,\n",
      "  \"parameter_count\": 459521,\n",
      "  \"dataset.split_ratio\": 0.2,\n",
      "  \"dataset_size.class_0\": 350,\n",
      "  \"dataset_size.class_1\": 349\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T21:59:53.574294Z",
     "start_time": "2025-02-07T21:59:53.572828Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
